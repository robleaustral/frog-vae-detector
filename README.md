# ğŸ¸ Frog Vocalization Detection using VAE

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)

Sistema de detecciÃ³n automÃ¡tica de vocalizaciones de ranas utilizando Variational Autoencoders (VAE) y clasificaciÃ³n supervisada.

## ğŸ“Š Resultados

| Modelo | Recall | PrecisiÃ³n | F1-Score |
|--------|--------|-----------|----------|
| VAE (40 muestras) | 90.00% | 36.73% | 52.17% |
| VAE (328 muestras) | 90.43% | 84.57% | 87.40% |
| **VAE + Clasificador** | **92.41%** | **94.92%** | **93.65%** âœ¨ |

## ğŸ—ï¸ Arquitectura

El sistema combina dos enfoques:

1. **VAE (Variational Autoencoder):** CompresiÃ³n no supervisada del espectrograma a espacio latente de 16 dimensiones
2. **Clasificador MLP:** Red neuronal supervisada para clasificaciÃ³n binaria (rana/no-rana)
```
Audio â†’ Mel-Spectrogram â†’ VAE Encoder â†’ Espacio Latente (16D) â†’ MLP â†’ PredicciÃ³n
```

## ğŸš€ InstalaciÃ³n

### Requisitos

- Python 3.8+
- PyTorch 2.0+
- librosa
- numpy
- matplotlib
- scikit-learn

### InstalaciÃ³n rÃ¡pida
```bash
# Clonar repositorio
git clone https://github.com/robleaustral/frog-vae-detector.git
cd frog-vae-detector

# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt
```

## ğŸ“¦ Estructura del Proyecto
```
frog-vae-detector/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ vae_model.py          # Arquitectura del VAE
â”‚   â”œâ”€â”€ audio_processor.py    # Preprocesamiento de audio
â”‚   â””â”€â”€ detector.py            # Sistema de detecciÃ³n
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_vae.py          # Entrenamiento del VAE
â”‚   â”œâ”€â”€ train_classifier.py   # Entrenamiento del clasificador
â”‚   â”œâ”€â”€ evaluate_model.py     # EvaluaciÃ³n de modelos
â”‚   â””â”€â”€ visualize_latent_space.py  # Visualizaciones
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ evaluation_notebook.ipynb  # AnÃ¡lisis interactivo
â”œâ”€â”€ figures/                   # Figuras del paper
â”œâ”€â”€ results/                   # MÃ©tricas y resultados
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ¯ Uso RÃ¡pido

### 1. Preprocesar Audio
```bash
python audio_preprocessor.py \
  -i ./data/raw \
  -o ./data/processed
```

### 2. Entrenar VAE
```bash
python scripts/train_vae.py \
  --data-dir ./data/processed \
  --output-dir ./trained_models \
  --epochs 100 \
  --latent-dim 16
```

### 3. Entrenar Clasificador
```bash
python scripts/train_classifier.py
```

### 4. Evaluar
```bash
python scripts/evaluate_model.py \
  --model-path ./trained_models/best_model.pth \
  --config-path ./trained_models/detector_config.json \
  --frog-data ./data/processed \
  --non-frog-data ./data/other_sounds_processed
```

## ğŸ“Š Dataset

El sistema fue evaluado con:

- **Ranas:** 328 segmentos de 5 segundos
  - 40 del dataset ESC-50
  - 288 de grabaciones de campo (12 audios de ~60 seg c/u)
- **No-ranas:** 120 segmentos (perros, gatos, aves del ESC-50)

## ğŸ”¬ MetodologÃ­a

### VAE (Variational Autoencoder)

- **Encoder:** Capas convolucionales que comprimen espectrograma (128Ã—216) a 16 dimensiones
- **Decoder:** Reconstruye el espectrograma original
- **DetecciÃ³n:** Distancia euclidiana al centroide del espacio latente

### Clasificador Supervisado

- **Arquitectura:** MLP (16 â†’ 32 â†’ 16 â†’ 1)
- **RegularizaciÃ³n:** Dropout 30%
- **Optimizador:** Adam (lr=0.001)
- **FunciÃ³n de pÃ©rdida:** Binary Cross-Entropy

## ğŸ“ˆ Visualizaciones

### Espacio Latente (PCA)

![Espacio Latente](figures/fig4_latent_space.png)

### EvoluciÃ³n del F1-Score

![F1 Evolution](figures/fig2_f1_evolution.png)

### ComparaciÃ³n con Estado del Arte

![State of Art](figures/fig3_state_of_art.png)

## ğŸ“„ CitaciÃ³n

Si utilizas este cÃ³digo en tu investigaciÃ³n, por favor cita:
```bibtex
@article{frog_vae_2025,
  title={Efficient Frog Vocalization Detection using Variational Autoencoders},
  author={Luis Veas-Castillo},
  journal={[JOURNAL]},
  year={2025}
}
```

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para detalles.

## ğŸ‘¥ Autores

- **Luis Veas-Castillo** - *Trabajo inicial* - [GitHub](https://github.com/robleaustral)

## ğŸ™ Agradecimientos

- Dataset ESC-50: Karol J. Piczak
- Comunidad de PyTorch
- [Otras instituciones/personas]

## ğŸ“§ Contacto

Para preguntas o colaboraciones: [luis.veasc@inf.uach.cl]

---

**Roble Austral Organization** - [GitHub](https://github.com/robleaustral)

## ğŸ‘¤ Autor

**Luis Veas-Castillo**
- Instituto de InformÃ¡tica
- Universidad Austral de Chile
- Valdivia, Chile
- ğŸ“§ luis.veasc@inf.uach.cl
